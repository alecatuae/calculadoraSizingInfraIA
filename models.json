{
  "models": [
    {
      "name": "opt-oss-120b",
      "num_layers": 36,
      "num_key_value_heads": 8,
      "head_dim": 64,
      "max_position_embeddings": 131072,
      "attention_pattern": "hybrid",
      "hybrid_full_layers": 18,
      "hybrid_sliding_layers": 18,
      "sliding_window": 128,
      "default_kv_precision": "fp8",
      "total_params_b": 120.0,
      "active_params_b": null,
      "weights_memory_gib_fp16": 240.0,
      "weights_memory_gib_fp8": 120.0,
      "weights_memory_gib_int8": 120.0,
      "weights_memory_gib_int4": 60.0,
      "default_weights_precision": "fp8",
      "notes": "Modelo 120B parâmetros. GQA com 8 KV heads, head_dim 64, max_position_embeddings 131072 e padrão híbrido (full+sliding) com sliding_window 128. Memória de pesos: FP16=240GiB, FP8/INT8=120GiB, INT4=60GiB."
    },
    {
      "name": "opt-oss-20b",
      "num_layers": 24,
      "num_key_value_heads": 8,
      "head_dim": 64,
      "max_position_embeddings": 131072,
      "attention_pattern": "hybrid",
      "hybrid_full_layers": 12,
      "hybrid_sliding_layers": 12,
      "sliding_window": 128,
      "default_kv_precision": "fp8",
      "total_params_b": 20.0,
      "active_params_b": null,
      "weights_memory_gib_fp16": 40.0,
      "weights_memory_gib_fp8": 20.0,
      "weights_memory_gib_int8": 20.0,
      "weights_memory_gib_int4": 10.0,
      "default_weights_precision": "fp8",
      "notes": "Modelo 20B parâmetros. 24 camadas, 8 KV heads, max_position_embeddings 131072, padrão híbrido (full+sliding). Memória de pesos: FP16=40GiB, FP8/INT8=20GiB, INT4=10GiB."
    }
  ]
}
