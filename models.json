{
  "models": [
    {
      "name": "opt-oss-120b",
      "num_layers": 36,
      "num_key_value_heads": 8,
      "head_dim": 64,
      "max_position_embeddings": 131072,
      "attention_pattern": "hybrid",
      "hybrid_full_layers": 18,
      "hybrid_sliding_layers": 18,
      "sliding_window": 128,
      "default_kv_precision": "fp8",
      "total_params_b": 120.0,
      "active_params_b": null,
      "weights_memory_gib_fp16": 240.0,
      "weights_memory_gib_fp8": 120.0,
      "weights_memory_gib_int8": 120.0,
      "weights_memory_gib_int4": 60.0,
      "default_weights_precision": "fp8",
      "notes": "Modelo 120B parâmetros. GQA com 8 KV heads, head_dim 64, max_position_embeddings 131072 e padrão híbrido (full+sliding) com sliding_window 128. Memória de pesos: FP16=240GiB, FP8/INT8=120GiB, INT4=60GiB."
    },
    {
      "name": "opt-oss-20b",
      "num_layers": 24,
      "num_key_value_heads": 8,
      "head_dim": 64,
      "max_position_embeddings": 131072,
      "attention_pattern": "hybrid",
      "hybrid_full_layers": 12,
      "hybrid_sliding_layers": 12,
      "sliding_window": 128,
      "default_kv_precision": "fp8",
      "total_params_b": 20.0,
      "active_params_b": null,
      "weights_memory_gib_fp16": 40.0,
      "weights_memory_gib_fp8": 20.0,
      "weights_memory_gib_int8": 20.0,
      "weights_memory_gib_int4": 10.0,
      "default_weights_precision": "fp8",
      "notes": "Modelo 20B parâmetros. 24 camadas, 8 KV heads, max_position_embeddings 131072, padrão híbrido (full+sliding). Memória de pesos: FP16=40GiB, FP8/INT8=20GiB, INT4=10GiB."
    },
    {
      "name": "DeepSeek-V3.2",
      "num_layers": 61,
      "num_key_value_heads": 128,
      "head_dim": 56,
      "max_position_embeddings": 163840,
      "attention_pattern": "sliding",
      "hybrid_full_layers": 0,
      "hybrid_sliding_layers": 61,
      "sliding_window": 2048,
      "default_kv_precision": "bf16",
      "total_params_b": 685.0,
      "active_params_b": 37.0,
      "weights_memory_gib_fp16": 1275.91,
      "weights_memory_gib_fp8": 637.96,
      "weights_memory_gib_int8": 637.96,
      "weights_memory_gib_int4": 318.98,
      "default_weights_precision": "fp8",
      "notes": "Campos DIRETOS do config: num_hidden_layers=61, num_attention_heads=128, num_key_value_heads=128, hidden_size=7168, max_position_embeddings=163840, quant_method=fp8 fmt=e4m3 e torch_dtype=bfloat16. head_dim=hidden_size/num_attention_heads=7168/128=56 (derivado). O modelo usa dims específicas: qk_rope_head_dim=64, qk_nope_head_dim=128 e v_head_dim=128 (MLA / atenção latente). Usa DSA (DeepSeek Sparse Attention) descrita no model card, mas para sizing de KV cache tratamos como sliding window. sliding_window=2048 é uma HEURÍSTICA (não está no config): escolhi 2048 por ser um valor comum de blocagem/top-k local em implementações esparsas e porque o config expõe index_topk=2048 (sinal de seleção esparsa). active_params_b=37B é uma estimativa por continuidade com DeepSeek-V3 (671B total, 37B ativados) + fato do V3.2 manter MoE com n_routed_experts=256, n_shared_experts=1 e num_experts_per_tok=8 no config. Memória de pesos foi calculada por: GiB = params * bytes / 2^30; FP16=2 bytes, FP8/INT8=1 byte, INT4=0.5 byte."
    },
    {
      "name": "DeepSeek-V3.2-Exp",
      "num_layers": 61,
      "num_key_value_heads": 128,
      "head_dim": 56,
      "max_position_embeddings": 163840,
      "attention_pattern": "sliding",
      "hybrid_full_layers": 0,
      "hybrid_sliding_layers": 61,
      "sliding_window": 2048,
      "default_kv_precision": "bf16",
      "total_params_b": 685.0,
      "active_params_b": 37.0,
      "weights_memory_gib_fp16": 1275.91,
      "weights_memory_gib_fp8": 637.96,
      "weights_memory_gib_int8": 637.96,
      "weights_memory_gib_int4": 318.98,
      "default_weights_precision": "fp8",
      "notes": "O config do DeepSeek-V3.2-Exp é (na prática) o mesmo do V3.2: 61 camadas, hidden_size=7168, 128 heads, 128 KV heads, max_position_embeddings=163840, quant_method=fp8 (e4m3) e torch_dtype=bfloat16. A release oficial da DeepSeek diz que o V3.2 é sucessor do V3.2-Exp e que a estrutura do V3.2/Speciale é a mesma do Exp. Usa DSA (sparse attention) mas para sizing tratamos como sliding window. sliding_window=2048 e active_params_b=37B seguem a mesma heurística/assunção do item acima."
    },
    {
      "name": "DeepSeek-V3.2-Speciale",
      "num_layers": 61,
      "num_key_value_heads": 128,
      "head_dim": 56,
      "max_position_embeddings": 163840,
      "attention_pattern": "sliding",
      "hybrid_full_layers": 0,
      "hybrid_sliding_layers": 61,
      "sliding_window": 2048,
      "default_kv_precision": "bf16",
      "total_params_b": 685.0,
      "active_params_b": 37.0,
      "weights_memory_gib_fp16": 1275.91,
      "weights_memory_gib_fp8": 637.96,
      "weights_memory_gib_int8": 637.96,
      "weights_memory_gib_int4": 318.98,
      "default_weights_precision": "fp8",
      "notes": "O config do DeepSeek-V3.2-Speciale que está público é idêntico ao do V3.2 (mesmos hiperparâmetros principais). A diferença do Speciale é de pós-treino/objetivo de raciocínio e (segundo o model card) ele não suporta tool-calling, mas estruturalmente o esqueleto do transformer+MoE é o mesmo. Usa DSA (sparse attention) mas para sizing tratamos como sliding window. active_params_b=37B e sliding_window=2048 seguem o mesmo racional/heurística."
    }
  ]
}
